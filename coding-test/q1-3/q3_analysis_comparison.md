# CSV Analysis Approach Comparison

So for this task, I implemented two different approaches to handle the CSV datasets based on their size. For the smaller 100K row dataset in `q1_analyze_customers.py`, I went with the standard approach where you just load the entire CSV into memory using `pd.read_csv()`. This is pretty straightforward since pandas has all the built-in methods like `.value_counts()` and `.nunique()` that make the analysis clean and simple. The whole dataset sits in RAM at around 50 MB, which is totally manageable on most systems. This approach works well when you're doing quick exploratory analysis and the data fits comfortably in memory.

For the larger 2M row dataset in `q2_analyze_customers_large.py`, I had to take a different approach because loading everything at once would use too much memory. Instead, I used pandas' chunk reading where it processes the CSV in batches of 50,000 rows at a time. The key difference here is that instead of using pandas' built-in aggregation methods, I manually aggregate using Python's Counter and set data structures. Each chunk gets processed incrementally and then I update the counters, track unique cities and countries, and check for duplicates as I go. The code is a bit more complex, but the memory usage stays consistent regardless of the file size. In practice, this used around 550 MB for the full 2M rows, which is about 50% less than if I had loaded everything at once.

As for the trade-off, the small file approach is simpler and faster but memory-intensive, while the chunk approach is more complex but scales indefinitely. As a general rule, I'd use standard pandas for anything under 100K rows, consider the environment for datasets between 100K and 1M rows, and definitely go with chunk processing for anything over 1M rows or when working with limited RAM.
