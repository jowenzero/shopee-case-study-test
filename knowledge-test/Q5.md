# How do you finetune the LLM model from raw?

In my current workflow, finetuning an LLM starts with collecting and preparing a high-quality dataset that represents the specific task we want the model to excel at. The raw data needs to be cleaned, formatted properly, and split into training and validation sets. For example, if we're finetuning a model to become a customer service agent for an e-commerce platform, we would gather thousands of real customer conversations, format them into prompt-response pairs, and ensure the data quality is consistent. There's also the case there the dataset needs to be properly balanced for both training and testing. If the dataset during training is filled with mainly 5* reviews and not much 1* reviews, the model would struggle at identifying those 1* reviews. Once the dataset is pre-processed and ready to use, we configure the training parameters such as learning rate and batch size, and begin the training and finetuning process. During training, we monitor the loss curves to ensure the model is learning without overfitting, which means checking that the validation loss decreases alongside the training loss.

Of course, there are different finetuning techniques we can use depending on our resources and requirements. Full finetuning updates all the model weights, but this is computationally expensive and requires significant GPU memory. This is why techniques like LoRA (Low-Rank Adaptation) have become something I use often in my workflow, as they only train a small set of additional parameters while keeping the base model frozen, making it much more efficient. After finetuning completes, we evaluate the model on a test set using metrics on a well prepared rubric, and conduct human evaluation to verify the outputs meet our quality standards. If the results aren't satisfactory, we iterate by adjusting hyperparameters, improving the dataset quality, or trying different finetuning approaches until we achieve the desired result.