# How do you ensure that your AI agent answers correctly?

There are plenty of ways we can evaluate the accuracy of an AI agent. The easiest way is through human evaluation, where a group of testers uses the AI agent and verifies the AI's answers in numerous ways. As an example, if we are doing product research on the latest Samsung phone, we would need to check if the research data the AI gives us is factual, if the formatting is good, and whether the research details meet the criteria that the researchers need. Once we know exactly what we need, it all comes down to iterative prompt engineering and strict output validation. These are the key components to ensure that AI agents answer correctly, by using proper prompt engineering techniques and effective validation to guarantee the output formatting is exactly what we need.

Of course, there are also other methods besides just human evaluation. Human can be time-consuming, resource-intensive, and prone to bias. This is why we can also rely on AI-based evaluation. For an AI evaluation to work effectively, we need a list of criteria for the expected output, known as a rubric. A rubric is essential for both human and AI evaluation to reach a consistent output and helps identify issues. Once the AI evaluation is calibrated to reach similar results as the human evaluators, it can take over most of the testing which improves the prompt engineering lifecycle.